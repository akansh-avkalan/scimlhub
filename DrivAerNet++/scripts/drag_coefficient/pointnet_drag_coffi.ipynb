{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cab553",
   "metadata": {},
   "source": [
    "# Drag Coff. prediction using Pointnet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7800ef",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfe9a8",
   "metadata": {},
   "source": [
    "### Mesh -> Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0da619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 STL files. Starting conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:04<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from tqdm import tqdm\n",
    "\n",
    "def mesh_to_pointcloud(stl_path, n_samples=4056):\n",
    "    \"\"\"\n",
    "    Converts an STL mesh to a normalized point cloud using \n",
    "    Surface Sampling followed by Farthest Point Sampling (FPS).\n",
    "    \"\"\"\n",
    "    # 1. Load Mesh\n",
    "    mesh = o3d.io.read_triangle_mesh(stl_path)\n",
    "    \n",
    "    # 2. Pre-process Mesh (Essential for DrivAerNet++ STLs)\n",
    "    mesh.remove_degenerate_triangles()\n",
    "    mesh.remove_duplicated_vertices()\n",
    "    \n",
    "    # 3. Surface Sampling \n",
    "    # We sample 5x more points than needed initially to give FPS a good 'pool' to pick from\n",
    "    temp_pcd = mesh.sample_points_uniformly(number_of_points=n_samples * 5)\n",
    "    pts = np.asarray(temp_pcd.points).astype(np.float32)\n",
    "    \n",
    "    # 4. Normalize (Center and Scale to Unit Sphere)\n",
    "    centroid = np.mean(pts, axis=0)\n",
    "    pts -= centroid\n",
    "    scale = np.max(np.linalg.norm(pts, axis=1))\n",
    "    pts /= (scale + 1e-9)\n",
    "    \n",
    "    # 5. Farthest Point Sampling (FPS) \n",
    "    # This ensures the 2048 points are spread perfectly over the car's geometry\n",
    "    fps_indices = farthest_point_sample_numpy(pts, n_samples)\n",
    "    final_pts = pts[fps_indices]\n",
    "    \n",
    "    return final_pts, centroid, scale\n",
    "\n",
    "def farthest_point_sample_numpy(points, n_samples):\n",
    "    \"\"\" Standard FPS implementation \"\"\"\n",
    "    N = points.shape[0]\n",
    "    centroids = np.zeros((n_samples,), dtype=np.int64)\n",
    "    distances = np.ones((N,), dtype=np.float32) * 1e10\n",
    "    farthest = np.random.randint(0, N)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        centroids[i] = farthest\n",
    "        centroid = points[farthest]\n",
    "        dist = np.sum((points - centroid) ** 2, axis=1)\n",
    "        mask = dist < distances\n",
    "        distances[mask] = dist[mask]\n",
    "        farthest = np.argmax(distances)\n",
    "    return centroids\n",
    "\n",
    "def process_dataset(input_dir, output_dir, n_points=2048):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    stl_files = sorted(glob.glob(os.path.join(input_dir, \"*.stl\")))\n",
    "    \n",
    "    print(f\"Found {len(stl_files)} STL files. Starting conversion...\")\n",
    "    \n",
    "    for stl_path in tqdm(stl_files):\n",
    "        fname = os.path.splitext(os.path.basename(stl_path))[0]\n",
    "        out_path = os.path.join(output_dir, f\"{fname}.npz\")\n",
    "        \n",
    "        if os.path.exists(out_path): continue\n",
    "        \n",
    "        try:\n",
    "            pts, center, scale = mesh_to_pointcloud(stl_path, n_samples=n_points)\n",
    "            \n",
    "            # Save as compressed numpy for PointNet training\n",
    "            np.savez(out_path, points=pts, centroid=center, scale=scale)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fname}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update these paths to your DrivAerNet++ folders\n",
    "    INPUT_FOLDER = \"02_dataset/mesh_new/F_D_WM_WW_1_subset\"\n",
    "    OUTPUT_FOLDER = \"02_dataset/car_4096_pc/pc_processed_2\"\n",
    "    \n",
    "    process_dataset(INPUT_FOLDER, OUTPUT_FOLDER, n_points=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0da20b",
   "metadata": {},
   "source": [
    "### Visualize the processed point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ca8d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "File /home/exouser/01_project/01_AE_triangular_mesh/02_dataset/car_design_mesh/pc_processed_2/F_D_WM_WW_0101.npz not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "\n",
    "def npz_to_ply(npz_path, output_ply_path, denormalize=True):\n",
    "    \"\"\"\n",
    "    Loads a processed .npz file and saves it as a .ply point cloud.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the input .npz file.\n",
    "        output_ply_path (str): Path where the .ply will be saved.\n",
    "        denormalize (bool): If True, uses saved centroid and scale to \n",
    "                           restore original car dimensions.\n",
    "    \"\"\"\n",
    "    # 1. Load the NPZ data\n",
    "    data = np.load(npz_path)\n",
    "    pts = data['points']      # Shape: (N, 3)\n",
    "    \n",
    "    # 2. Denormalize (Optional)\n",
    "    # This moves the car back from the \"unit sphere\" to its original CAD coordinates\n",
    "    if denormalize and 'centroid' in data and 'scale' in data:\n",
    "        centroid = data['centroid']\n",
    "        scale = data['scale']\n",
    "        pts = (pts * scale) + centroid\n",
    "        print(f\"Denormalized using scale: {scale:.4f}\")\n",
    "\n",
    "    # 3. Create Open3D PointCloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "\n",
    "    # 4. Save to PLY\n",
    "    o3d.io.write_point_cloud(output_ply_path, pcd)\n",
    "    print(f\"Successfully saved PLY to: {output_ply_path}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Change these to your actual paths\n",
    "    INPUT_NPZ = \"/home/exouser/01_project/01_AE_triangular_mesh/02_dataset/car_design_mesh/pc_processed_2/F_D_WM_WW_0101.npz\"\n",
    "    OUTPUT_PLY = \"pc_car_4098_denormalised.ply\"\n",
    "\n",
    "    if os.path.exists(INPUT_NPZ):\n",
    "        npz_to_ply(INPUT_NPZ, OUTPUT_PLY, denormalize=False)\n",
    "    else:\n",
    "        print(f\"File {INPUT_NPZ} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd81476",
   "metadata": {},
   "source": [
    "### Labels Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7bcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ID  Drag_Value\n",
      "0  F_D_WM_WW_0001    0.275833\n",
      "1  F_D_WM_WW_0002    0.279602\n",
      "2  F_D_WM_WW_0003    0.272837\n",
      "3  F_D_WM_WW_0004    0.257712\n",
      "4  F_D_WM_WW_0005    0.267890\n",
      "Total samples: 300\n"
     ]
    }
   ],
   "source": [
    "RAW_CSV_PATH = \"02_dataset/cd_data_303/DrivAerNetPlusPlus_Cd_303.csv\"   # ID, Drag_Value\n",
    "\n",
    "df = pd.read_csv(RAW_CSV_PATH)\n",
    "print(df.head())\n",
    "print(f\"Total samples: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO   = 0.15\n",
    "TEST_RATIO  = 0.15\n",
    "\n",
    "assert abs(TRAIN_RATIO + VAL_RATIO + TEST_RATIO - 1.0) < 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "448e6afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 209\n",
      "Val samples:   45\n",
      "Test samples:  46\n"
     ]
    }
   ],
   "source": [
    "# First split: Train vs (Val + Test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=(1.0 - TRAIN_RATIO),\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: Val vs Test\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=TEST_RATIO / (VAL_RATIO + TEST_RATIO),\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Val samples:   {len(val_df)}\")\n",
    "print(f\"Test samples:  {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f64b4dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cd_min (train): 0.2406969401668661\n",
      "Cd_max (train): 0.3168595800557837\n"
     ]
    }
   ],
   "source": [
    "cd_min = train_df[\"Drag_Value\"].min()\n",
    "cd_max = train_df[\"Drag_Value\"].max()\n",
    "\n",
    "print(\"Cd_min (train):\", cd_min)\n",
    "print(\"Cd_max (train):\", cd_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved normalization metadata → drag_normalization.json\n"
     ]
    }
   ],
   "source": [
    "norm_metadata = {\n",
    "    \"Cd_min\": float(cd_min),\n",
    "    \"Cd_max\": float(cd_max)\n",
    "}\n",
    "\n",
    "with open(\"02_dataset/cd_data_303/drag_normalization.json\", \"w\") as f:\n",
    "    json.dump(norm_metadata, f, indent=4)\n",
    "\n",
    "print(\"Saved normalization metadata → drag_normalization.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7449e393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train normalized range: 0.0 1.0\n",
      "Train normalized range: 0.0 1.0\n",
      "Train normalized range: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalize(cd, cd_min, cd_max):\n",
    "    return (cd - cd_min) / (cd_max - cd_min)\n",
    "for split_df in [train_df, val_df, test_df]:\n",
    "    split_df[\"Drag_Value_Norm\"] = min_max_normalize(\n",
    "        split_df[\"Drag_Value\"],\n",
    "        cd_min,\n",
    "        cd_max\n",
    "    )\n",
    "\n",
    "    print(\"Train normalized range:\",\n",
    "      train_df[\"Drag_Value_Norm\"].min(),\n",
    "      train_df[\"Drag_Value_Norm\"].max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d30389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = train_df[[\"ID\", \"Drag_Value_Norm\"]]\n",
    "val_out   = val_df[[\"ID\", \"Drag_Value_Norm\"]]\n",
    "test_out  = test_df[[\"ID\", \"Drag_Value_Norm\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0187552b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- drag_train_normalized.csv\n",
      "- drag_val_normalized.csv\n",
      "- drag_test_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "train_out.to_csv(\"02_dataset/cd_data_303/drag_train_normalized.csv\", index=False)\n",
    "val_out.to_csv(\"02_dataset/cd_data_303/drag_val_normalized.csv\", index=False)\n",
    "test_out.to_csv(\"02_dataset/cd_data_303/drag_test_normalized.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"- drag_train_normalized.csv\")\n",
    "print(\"- drag_val_normalized.csv\")\n",
    "print(\"- drag_test_normalized.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08a5b4",
   "metadata": {},
   "source": [
    "## PointNet:\n",
    "- Input : Point Cloud. \n",
    "- 6 Convolution layers. \n",
    "- Channel increase: 64, 128, 256, 256, 512 Upto embedding dim (1024).\n",
    "- Batch Normalisation.\n",
    "- Residual connection link input -> Aiding gradient flow.\n",
    "- 3 Linear layers reduce: 1024 -> 512. \n",
    "- Finally single output for drag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642292e",
   "metadata": {},
   "source": [
    "## Data Loading and Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd692d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0af106ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ea8684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2670126",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eefcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrivAerPointNetDataset(Dataset):\n",
    "    def __init__(self, csv_file, pointcloud_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to normalized CSV (train/val/test)\n",
    "            pointcloud_dir (str): Directory containing .npz point clouds\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.pointcloud_dir = pointcloud_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        sample_id = row[\"ID\"]\n",
    "        cd = row[\"Drag_Value_Norm\"]  # already normalized\n",
    "\n",
    "        pc_path = os.path.join(self.pointcloud_dir, f\"{sample_id}.npz\")\n",
    "        pc_data = np.load(pc_path)\n",
    "\n",
    "        points = pc_data[\"points\"]  # (N, 3)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        points = torch.from_numpy(points).float()      # (N, 3)\n",
    "        cd = torch.tensor(cd).float()                   # scalar\n",
    "\n",
    "        return points, cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02cb9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTCLOUD_DIR = \"02_dataset/car_4096_pc/pc_processed\"\n",
    "CSV_DIR = \"02_dataset/cd_data_303\"\n",
    "\n",
    "train_dataset = DrivAerPointNetDataset(\n",
    "    csv_file=os.path.join(CSV_DIR, \"drag_train_normalized.csv\"),\n",
    "    pointcloud_dir=POINTCLOUD_DIR\n",
    ")\n",
    "\n",
    "val_dataset = DrivAerPointNetDataset(\n",
    "    csv_file=os.path.join(CSV_DIR, \"drag_val_normalized.csv\"),\n",
    "    pointcloud_dir=POINTCLOUD_DIR\n",
    ")\n",
    "\n",
    "test_dataset = DrivAerPointNetDataset(\n",
    "    csv_file=os.path.join(CSV_DIR, \"drag_test_normalized.csv\"),\n",
    "    pointcloud_dir=POINTCLOUD_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a12df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8   # adjust based on GPU memory\n",
    "NUM_WORKERS = 4  # set 0 if debugging\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b81119c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points shape: torch.Size([8, 4096, 3])\n",
      "Cd shape: torch.Size([8])\n",
      "Cd range: 0.2213650643825531 0.7796438336372375\n"
     ]
    }
   ],
   "source": [
    "points, cd = next(iter(train_loader))\n",
    "\n",
    "print(\"Points shape:\", points.shape)  # (B, N, 3)\n",
    "print(\"Cd shape:\", cd.shape)          # (B,)\n",
    "print(\"Cd range:\", cd.min().item(), cd.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d78f0",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "984bb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54c8ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetBackbone(nn.Module):\n",
    "    def __init__(self, emb_dim=1024):\n",
    "        super().__init__()\n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 1)\n",
    "        self.conv4 = nn.Conv1d(256, 256, 1)\n",
    "        self.conv5 = nn.Conv1d(256, 512, 1)\n",
    "        self.conv6 = nn.Conv1d(512, emb_dim, 1)\n",
    "        \n",
    "        # BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.bn6 = nn.BatchNorm1d(emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, N, 3)\n",
    "        x = x.transpose(2, 1)  # (B, 3, N)\n",
    "\n",
    "        # Conv + BN + ReLU\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))  # 64 channels\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1))) # 128\n",
    "        x3 = F.relu(self.bn3(self.conv3(x2))) # 256\n",
    "        x4 = F.relu(self.bn4(self.conv4(x3))) # 256\n",
    "        x5 = F.relu(self.bn5(self.conv5(x4))) # 512\n",
    "        x6 = self.bn6(self.conv6(x5))         # 1024, no ReLU here (residual will be added)\n",
    "        \n",
    "        return x1, x6  # return early feat for residual + deep embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2da2be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetRegressor(nn.Module):\n",
    "    def __init__(self, emb_dim=1024):\n",
    "        super().__init__()\n",
    "        self.backbone = PointNetBackbone(emb_dim=emb_dim)\n",
    "        \n",
    "        # Residual projection: 64 → 1024\n",
    "        self.res_proj = nn.Conv1d(64, emb_dim, 1)\n",
    "        \n",
    "        # Fully connected regression head\n",
    "        self.fc1 = nn.Linear(emb_dim, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, N, 3)\n",
    "        early_feat, deep_feat = self.backbone(x)  # x1, x6\n",
    "        \n",
    "        # Residual connection\n",
    "        res = self.res_proj(early_feat)\n",
    "        feat = deep_feat + res\n",
    "        \n",
    "        # Global max pooling\n",
    "        global_feat, _ = torch.max(feat, 2)  # (B, 1024)\n",
    "        \n",
    "        # FC regression\n",
    "        x = F.relu(self.bn_fc1(self.fc1(global_feat)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x.squeeze(1)  # (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04d34e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8])\n",
      "Sample outputs: tensor([ 0.7969,  0.0417,  0.6766, -0.2740,  0.4204], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = PointNetRegressor(emb_dim=1024)\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Grab one batch\n",
    "points, cd = next(iter(train_loader))\n",
    "points = points.to(model.fc1.weight.device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(points)\n",
    "print(\"Output shape:\", output.shape)  # Expected: (B,)\n",
    "print(\"Sample outputs:\", output[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc2f41",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0116e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "580e1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CHECKPOINT_DIR = \"checkpoints/pc_dc_pn\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "LOG_FILE = \"logs/training_log.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93e79e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_EVERY = 10  # save checkpoint every 10 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e38ad9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = PointNetRegressor(emb_dim=1024).to(DEVICE)\n",
    "\n",
    "# Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d06665c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_90.pth\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "checkpoint_files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.endswith(\".pth\")])\n",
    "\n",
    "if checkpoint_files:\n",
    "    latest_ckpt = os.path.join(CHECKPOINT_DIR, checkpoint_files[-1])\n",
    "    print(f\"Resuming from checkpoint: {latest_ckpt}\")\n",
    "    \n",
    "    ckpt = torch.load(latest_ckpt, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "else:\n",
    "    print(\"No checkpoint found, starting from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09205bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/500]  Train Loss: 0.015663  Val Loss: 0.872477\n",
      "Epoch [92/500]  Train Loss: 0.018823  Val Loss: 0.269228\n",
      "Epoch [93/500]  Train Loss: 0.011082  Val Loss: 0.303799\n",
      "Epoch [94/500]  Train Loss: 0.019216  Val Loss: 1.271782\n",
      "Epoch [95/500]  Train Loss: 0.019308  Val Loss: 0.056256\n",
      "Epoch [96/500]  Train Loss: 0.016813  Val Loss: 2.154948\n",
      "Epoch [97/500]  Train Loss: 0.016059  Val Loss: 0.227945\n",
      "Epoch [98/500]  Train Loss: 0.014917  Val Loss: 0.139216\n",
      "Epoch [99/500]  Train Loss: 0.012038  Val Loss: 0.060865\n",
      "Epoch [100/500]  Train Loss: 0.009778  Val Loss: 0.045446\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_100.pth\n",
      "Epoch [101/500]  Train Loss: 0.012941  Val Loss: 0.404086\n",
      "Epoch [102/500]  Train Loss: 0.016186  Val Loss: 0.112257\n",
      "Epoch [103/500]  Train Loss: 0.012915  Val Loss: 0.430439\n",
      "Epoch [104/500]  Train Loss: 0.018836  Val Loss: 0.237765\n",
      "Epoch [105/500]  Train Loss: 0.015643  Val Loss: 0.138575\n",
      "Epoch [106/500]  Train Loss: 0.011903  Val Loss: 0.365458\n",
      "Epoch [107/500]  Train Loss: 0.013248  Val Loss: 0.096071\n",
      "Epoch [108/500]  Train Loss: 0.011027  Val Loss: 0.162965\n",
      "Epoch [109/500]  Train Loss: 0.016074  Val Loss: 0.769532\n",
      "Epoch [110/500]  Train Loss: 0.015628  Val Loss: 0.345938\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_110.pth\n",
      "Epoch [111/500]  Train Loss: 0.011825  Val Loss: 0.088412\n",
      "Epoch [112/500]  Train Loss: 0.015885  Val Loss: 0.238935\n",
      "Epoch [113/500]  Train Loss: 0.022264  Val Loss: 1.098011\n",
      "Epoch [114/500]  Train Loss: 0.015032  Val Loss: 0.005051\n",
      "Epoch [115/500]  Train Loss: 0.012539  Val Loss: 0.063724\n",
      "Epoch [116/500]  Train Loss: 0.009949  Val Loss: 0.712291\n",
      "Epoch [117/500]  Train Loss: 0.012803  Val Loss: 0.238653\n",
      "Epoch [118/500]  Train Loss: 0.012547  Val Loss: 1.324677\n",
      "Epoch [119/500]  Train Loss: 0.015345  Val Loss: 0.463766\n",
      "Epoch [120/500]  Train Loss: 0.010398  Val Loss: 0.241566\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_120.pth\n",
      "Epoch [121/500]  Train Loss: 0.012592  Val Loss: 0.683300\n",
      "Epoch [122/500]  Train Loss: 0.012443  Val Loss: 0.152625\n",
      "Epoch [123/500]  Train Loss: 0.013309  Val Loss: 0.202749\n",
      "Epoch [124/500]  Train Loss: 0.012177  Val Loss: 0.308892\n",
      "Epoch [125/500]  Train Loss: 0.014689  Val Loss: 0.149385\n",
      "Epoch [126/500]  Train Loss: 0.007346  Val Loss: 0.806291\n",
      "Epoch [127/500]  Train Loss: 0.011253  Val Loss: 0.045082\n",
      "Epoch [128/500]  Train Loss: 0.024664  Val Loss: 0.267575\n",
      "Epoch [129/500]  Train Loss: 0.010068  Val Loss: 2.100072\n",
      "Epoch [130/500]  Train Loss: 0.016158  Val Loss: 0.362622\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_130.pth\n",
      "Epoch [131/500]  Train Loss: 0.017074  Val Loss: 0.350240\n",
      "Epoch [132/500]  Train Loss: 0.018296  Val Loss: 0.184617\n",
      "Epoch [133/500]  Train Loss: 0.012431  Val Loss: 0.443730\n",
      "Epoch [134/500]  Train Loss: 0.010893  Val Loss: 1.411795\n",
      "Epoch [135/500]  Train Loss: 0.011180  Val Loss: 0.513831\n",
      "Epoch [136/500]  Train Loss: 0.021742  Val Loss: 0.478256\n",
      "Epoch [137/500]  Train Loss: 0.021190  Val Loss: 0.222942\n",
      "Epoch [138/500]  Train Loss: 0.014896  Val Loss: 0.246810\n",
      "Epoch [139/500]  Train Loss: 0.010123  Val Loss: 1.063281\n",
      "Epoch [140/500]  Train Loss: 0.010255  Val Loss: 0.363520\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_140.pth\n",
      "Epoch [141/500]  Train Loss: 0.012105  Val Loss: 0.151138\n",
      "Epoch [142/500]  Train Loss: 0.012136  Val Loss: 1.365739\n",
      "Epoch [143/500]  Train Loss: 0.012268  Val Loss: 0.077268\n",
      "Epoch [144/500]  Train Loss: 0.010925  Val Loss: 0.705169\n",
      "Epoch [145/500]  Train Loss: 0.010683  Val Loss: 0.029773\n",
      "Epoch [146/500]  Train Loss: 0.009811  Val Loss: 1.325300\n",
      "Epoch [147/500]  Train Loss: 0.010527  Val Loss: 0.117718\n",
      "Epoch [148/500]  Train Loss: 0.010004  Val Loss: 0.078228\n",
      "Epoch [149/500]  Train Loss: 0.013542  Val Loss: 1.088886\n",
      "Epoch [150/500]  Train Loss: 0.013096  Val Loss: 0.573069\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_150.pth\n",
      "Epoch [151/500]  Train Loss: 0.018066  Val Loss: 0.474082\n",
      "Epoch [152/500]  Train Loss: 0.009141  Val Loss: 1.068581\n",
      "Epoch [153/500]  Train Loss: 0.012457  Val Loss: 0.256990\n",
      "Epoch [154/500]  Train Loss: 0.010280  Val Loss: 1.166841\n",
      "Epoch [155/500]  Train Loss: 0.009205  Val Loss: 0.842807\n",
      "Epoch [156/500]  Train Loss: 0.009763  Val Loss: 0.090297\n",
      "Epoch [157/500]  Train Loss: 0.011333  Val Loss: 0.670616\n",
      "Epoch [158/500]  Train Loss: 0.011744  Val Loss: 0.029044\n",
      "Epoch [159/500]  Train Loss: 0.011292  Val Loss: 0.310912\n",
      "Epoch [160/500]  Train Loss: 0.009650  Val Loss: 0.858933\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_160.pth\n",
      "Epoch [161/500]  Train Loss: 0.010576  Val Loss: 0.340348\n",
      "Epoch [162/500]  Train Loss: 0.012119  Val Loss: 0.315858\n",
      "Epoch [163/500]  Train Loss: 0.011303  Val Loss: 1.027026\n",
      "Epoch [164/500]  Train Loss: 0.012180  Val Loss: 2.302987\n",
      "Epoch [165/500]  Train Loss: 0.011856  Val Loss: 0.199967\n",
      "Epoch [166/500]  Train Loss: 0.014258  Val Loss: 0.911653\n",
      "Epoch [167/500]  Train Loss: 0.011048  Val Loss: 0.209527\n",
      "Epoch [168/500]  Train Loss: 0.021025  Val Loss: 0.274631\n",
      "Epoch [169/500]  Train Loss: 0.014198  Val Loss: 0.226341\n",
      "Epoch [170/500]  Train Loss: 0.010987  Val Loss: 0.283766\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_170.pth\n",
      "Epoch [171/500]  Train Loss: 0.010323  Val Loss: 0.126363\n",
      "Epoch [172/500]  Train Loss: 0.009578  Val Loss: 0.637273\n",
      "Epoch [173/500]  Train Loss: 0.013835  Val Loss: 0.301769\n",
      "Epoch [174/500]  Train Loss: 0.020359  Val Loss: 0.037411\n",
      "Epoch [175/500]  Train Loss: 0.014173  Val Loss: 0.322526\n",
      "Epoch [176/500]  Train Loss: 0.011055  Val Loss: 1.079611\n",
      "Epoch [177/500]  Train Loss: 0.011591  Val Loss: 0.042989\n",
      "Epoch [178/500]  Train Loss: 0.013377  Val Loss: 2.624805\n",
      "Epoch [179/500]  Train Loss: 0.009396  Val Loss: 0.402092\n",
      "Epoch [180/500]  Train Loss: 0.009544  Val Loss: 0.062653\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_180.pth\n",
      "Epoch [181/500]  Train Loss: 0.008875  Val Loss: 0.465435\n",
      "Epoch [182/500]  Train Loss: 0.011503  Val Loss: 1.146714\n",
      "Epoch [183/500]  Train Loss: 0.010390  Val Loss: 1.511931\n",
      "Epoch [184/500]  Train Loss: 0.012424  Val Loss: 0.196990\n",
      "Epoch [185/500]  Train Loss: 0.011102  Val Loss: 1.256502\n",
      "Epoch [186/500]  Train Loss: 0.010626  Val Loss: 0.309600\n",
      "Epoch [187/500]  Train Loss: 0.014763  Val Loss: 1.294256\n",
      "Epoch [188/500]  Train Loss: 0.012696  Val Loss: 0.113289\n",
      "Epoch [189/500]  Train Loss: 0.017046  Val Loss: 0.779750\n",
      "Epoch [190/500]  Train Loss: 0.010339  Val Loss: 1.361779\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_190.pth\n",
      "Epoch [191/500]  Train Loss: 0.008513  Val Loss: 0.101183\n",
      "Epoch [192/500]  Train Loss: 0.009003  Val Loss: 0.051590\n",
      "Epoch [193/500]  Train Loss: 0.009388  Val Loss: 0.770135\n",
      "Epoch [194/500]  Train Loss: 0.009679  Val Loss: 1.339853\n",
      "Epoch [195/500]  Train Loss: 0.008270  Val Loss: 0.284698\n",
      "Epoch [196/500]  Train Loss: 0.008730  Val Loss: 1.037781\n",
      "Epoch [197/500]  Train Loss: 0.009174  Val Loss: 0.019313\n",
      "Epoch [198/500]  Train Loss: 0.012141  Val Loss: 0.054712\n",
      "Epoch [199/500]  Train Loss: 0.011503  Val Loss: 0.132129\n",
      "Epoch [200/500]  Train Loss: 0.012014  Val Loss: 1.688487\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_200.pth\n",
      "Epoch [201/500]  Train Loss: 0.012299  Val Loss: 0.086086\n",
      "Epoch [202/500]  Train Loss: 0.008939  Val Loss: 0.454646\n",
      "Epoch [203/500]  Train Loss: 0.011120  Val Loss: 0.333639\n",
      "Epoch [204/500]  Train Loss: 0.008613  Val Loss: 0.585114\n",
      "Epoch [205/500]  Train Loss: 0.007451  Val Loss: 0.853238\n",
      "Epoch [206/500]  Train Loss: 0.012765  Val Loss: 0.574621\n",
      "Epoch [207/500]  Train Loss: 0.011257  Val Loss: 0.788033\n",
      "Epoch [208/500]  Train Loss: 0.012712  Val Loss: 0.172464\n",
      "Epoch [209/500]  Train Loss: 0.015451  Val Loss: 0.137573\n",
      "Epoch [210/500]  Train Loss: 0.009613  Val Loss: 2.017364\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_210.pth\n",
      "Epoch [211/500]  Train Loss: 0.010225  Val Loss: 1.308719\n",
      "Epoch [212/500]  Train Loss: 0.011380  Val Loss: 0.420871\n",
      "Epoch [213/500]  Train Loss: 0.009288  Val Loss: 0.543938\n",
      "Epoch [214/500]  Train Loss: 0.010387  Val Loss: 0.220851\n",
      "Epoch [215/500]  Train Loss: 0.010163  Val Loss: 0.796440\n",
      "Epoch [216/500]  Train Loss: 0.009615  Val Loss: 0.398711\n",
      "Epoch [217/500]  Train Loss: 0.015529  Val Loss: 0.772683\n",
      "Epoch [218/500]  Train Loss: 0.011362  Val Loss: 2.629353\n",
      "Epoch [219/500]  Train Loss: 0.009768  Val Loss: 0.178723\n",
      "Epoch [220/500]  Train Loss: 0.009174  Val Loss: 0.081801\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_220.pth\n",
      "Epoch [221/500]  Train Loss: 0.009877  Val Loss: 0.284989\n",
      "Epoch [222/500]  Train Loss: 0.010181  Val Loss: 0.035118\n",
      "Epoch [223/500]  Train Loss: 0.010400  Val Loss: 0.130359\n",
      "Epoch [224/500]  Train Loss: 0.008842  Val Loss: 1.111622\n",
      "Epoch [225/500]  Train Loss: 0.011866  Val Loss: 0.024066\n",
      "Epoch [226/500]  Train Loss: 0.016582  Val Loss: 0.219572\n",
      "Epoch [227/500]  Train Loss: 0.013985  Val Loss: 1.585631\n",
      "Epoch [228/500]  Train Loss: 0.016355  Val Loss: 0.966239\n",
      "Epoch [229/500]  Train Loss: 0.011078  Val Loss: 0.353524\n",
      "Epoch [230/500]  Train Loss: 0.012395  Val Loss: 0.088311\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_230.pth\n",
      "Epoch [231/500]  Train Loss: 0.013785  Val Loss: 0.293048\n",
      "Epoch [232/500]  Train Loss: 0.008651  Val Loss: 0.174495\n",
      "Epoch [233/500]  Train Loss: 0.012086  Val Loss: 0.256976\n",
      "Epoch [234/500]  Train Loss: 0.011755  Val Loss: 0.065624\n",
      "Epoch [235/500]  Train Loss: 0.007572  Val Loss: 0.020100\n",
      "Epoch [236/500]  Train Loss: 0.009614  Val Loss: 0.941030\n",
      "Epoch [237/500]  Train Loss: 0.011076  Val Loss: 0.846139\n",
      "Epoch [238/500]  Train Loss: 0.008245  Val Loss: 0.066197\n",
      "Epoch [239/500]  Train Loss: 0.007959  Val Loss: 0.690378\n",
      "Epoch [240/500]  Train Loss: 0.011843  Val Loss: 0.247419\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_240.pth\n",
      "Epoch [241/500]  Train Loss: 0.008334  Val Loss: 2.061651\n",
      "Epoch [242/500]  Train Loss: 0.010034  Val Loss: 0.050573\n",
      "Epoch [243/500]  Train Loss: 0.012494  Val Loss: 0.591419\n",
      "Epoch [244/500]  Train Loss: 0.008064  Val Loss: 0.160155\n",
      "Epoch [245/500]  Train Loss: 0.008369  Val Loss: 0.050328\n",
      "Epoch [246/500]  Train Loss: 0.009640  Val Loss: 0.259347\n",
      "Epoch [247/500]  Train Loss: 0.007277  Val Loss: 0.255696\n",
      "Epoch [248/500]  Train Loss: 0.008783  Val Loss: 0.091008\n",
      "Epoch [249/500]  Train Loss: 0.008539  Val Loss: 0.181120\n",
      "Epoch [250/500]  Train Loss: 0.008084  Val Loss: 0.295415\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_250.pth\n",
      "Epoch [251/500]  Train Loss: 0.010335  Val Loss: 2.386545\n",
      "Epoch [252/500]  Train Loss: 0.010494  Val Loss: 0.268564\n",
      "Epoch [253/500]  Train Loss: 0.008204  Val Loss: 0.014068\n",
      "Epoch [254/500]  Train Loss: 0.009802  Val Loss: 0.176019\n",
      "Epoch [255/500]  Train Loss: 0.008435  Val Loss: 0.548464\n",
      "Epoch [256/500]  Train Loss: 0.007457  Val Loss: 2.487016\n",
      "Epoch [257/500]  Train Loss: 0.010149  Val Loss: 0.967506\n",
      "Epoch [258/500]  Train Loss: 0.009265  Val Loss: 0.675171\n",
      "Epoch [259/500]  Train Loss: 0.009215  Val Loss: 0.084337\n",
      "Epoch [260/500]  Train Loss: 0.007929  Val Loss: 0.565380\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_260.pth\n",
      "Epoch [261/500]  Train Loss: 0.009022  Val Loss: 0.167457\n",
      "Epoch [262/500]  Train Loss: 0.009764  Val Loss: 0.725821\n",
      "Epoch [263/500]  Train Loss: 0.008675  Val Loss: 0.963661\n",
      "Epoch [264/500]  Train Loss: 0.008411  Val Loss: 0.363239\n",
      "Epoch [265/500]  Train Loss: 0.008354  Val Loss: 0.839272\n",
      "Epoch [266/500]  Train Loss: 0.012990  Val Loss: 0.086613\n",
      "Epoch [267/500]  Train Loss: 0.009576  Val Loss: 0.496394\n",
      "Epoch [268/500]  Train Loss: 0.009416  Val Loss: 0.501666\n",
      "Epoch [269/500]  Train Loss: 0.007714  Val Loss: 0.734195\n",
      "Epoch [270/500]  Train Loss: 0.010020  Val Loss: 0.217709\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_270.pth\n",
      "Epoch [271/500]  Train Loss: 0.014544  Val Loss: 0.398084\n",
      "Epoch [272/500]  Train Loss: 0.011408  Val Loss: 1.270945\n",
      "Epoch [273/500]  Train Loss: 0.013586  Val Loss: 3.295889\n",
      "Epoch [274/500]  Train Loss: 0.013558  Val Loss: 0.098341\n",
      "Epoch [275/500]  Train Loss: 0.010596  Val Loss: 0.633025\n",
      "Epoch [276/500]  Train Loss: 0.008950  Val Loss: 0.032287\n",
      "Epoch [277/500]  Train Loss: 0.008819  Val Loss: 2.924939\n",
      "Epoch [278/500]  Train Loss: 0.007934  Val Loss: 0.304109\n",
      "Epoch [279/500]  Train Loss: 0.008924  Val Loss: 0.016055\n",
      "Epoch [280/500]  Train Loss: 0.019757  Val Loss: 0.474449\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_280.pth\n",
      "Epoch [281/500]  Train Loss: 0.012145  Val Loss: 0.316782\n",
      "Epoch [282/500]  Train Loss: 0.011361  Val Loss: 1.560204\n",
      "Epoch [283/500]  Train Loss: 0.010005  Val Loss: 0.224467\n",
      "Epoch [284/500]  Train Loss: 0.011581  Val Loss: 0.161327\n",
      "Epoch [285/500]  Train Loss: 0.008116  Val Loss: 0.207637\n",
      "Epoch [286/500]  Train Loss: 0.009579  Val Loss: 0.354037\n",
      "Epoch [287/500]  Train Loss: 0.012364  Val Loss: 0.315224\n",
      "Epoch [288/500]  Train Loss: 0.011113  Val Loss: 0.185293\n",
      "Epoch [289/500]  Train Loss: 0.007336  Val Loss: 1.088127\n",
      "Epoch [290/500]  Train Loss: 0.008365  Val Loss: 0.263523\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_290.pth\n",
      "Epoch [291/500]  Train Loss: 0.008821  Val Loss: 0.265599\n",
      "Epoch [292/500]  Train Loss: 0.005950  Val Loss: 1.703505\n",
      "Epoch [293/500]  Train Loss: 0.007746  Val Loss: 0.272251\n",
      "Epoch [294/500]  Train Loss: 0.008431  Val Loss: 3.114777\n",
      "Epoch [295/500]  Train Loss: 0.011886  Val Loss: 5.742365\n",
      "Epoch [296/500]  Train Loss: 0.018292  Val Loss: 1.397894\n",
      "Epoch [297/500]  Train Loss: 0.010378  Val Loss: 1.095152\n",
      "Epoch [298/500]  Train Loss: 0.008903  Val Loss: 0.657771\n",
      "Epoch [299/500]  Train Loss: 0.013588  Val Loss: 0.783997\n",
      "Epoch [300/500]  Train Loss: 0.009349  Val Loss: 1.020681\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_300.pth\n",
      "Epoch [301/500]  Train Loss: 0.008635  Val Loss: 2.131858\n",
      "Epoch [302/500]  Train Loss: 0.008893  Val Loss: 0.188693\n",
      "Epoch [303/500]  Train Loss: 0.012336  Val Loss: 0.728569\n",
      "Epoch [304/500]  Train Loss: 0.010109  Val Loss: 1.798546\n",
      "Epoch [305/500]  Train Loss: 0.008676  Val Loss: 0.302925\n",
      "Epoch [306/500]  Train Loss: 0.008332  Val Loss: 0.087273\n",
      "Epoch [307/500]  Train Loss: 0.007869  Val Loss: 0.902181\n",
      "Epoch [308/500]  Train Loss: 0.010056  Val Loss: 0.072728\n",
      "Epoch [309/500]  Train Loss: 0.007371  Val Loss: 1.610771\n",
      "Epoch [310/500]  Train Loss: 0.011052  Val Loss: 0.035755\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_310.pth\n",
      "Epoch [311/500]  Train Loss: 0.008527  Val Loss: 0.485853\n",
      "Epoch [312/500]  Train Loss: 0.009365  Val Loss: 0.129286\n",
      "Epoch [313/500]  Train Loss: 0.013926  Val Loss: 0.401347\n",
      "Epoch [314/500]  Train Loss: 0.012140  Val Loss: 0.317836\n",
      "Epoch [315/500]  Train Loss: 0.012594  Val Loss: 0.202635\n",
      "Epoch [316/500]  Train Loss: 0.011976  Val Loss: 2.509334\n",
      "Epoch [317/500]  Train Loss: 0.006816  Val Loss: 0.500191\n",
      "Epoch [318/500]  Train Loss: 0.007399  Val Loss: 2.291777\n",
      "Epoch [319/500]  Train Loss: 0.008060  Val Loss: 0.169943\n",
      "Epoch [320/500]  Train Loss: 0.008499  Val Loss: 0.061578\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_320.pth\n",
      "Epoch [321/500]  Train Loss: 0.009658  Val Loss: 1.731490\n",
      "Epoch [322/500]  Train Loss: 0.006327  Val Loss: 0.067626\n",
      "Epoch [323/500]  Train Loss: 0.009424  Val Loss: 0.245465\n",
      "Epoch [324/500]  Train Loss: 0.009125  Val Loss: 0.762106\n",
      "Epoch [325/500]  Train Loss: 0.007489  Val Loss: 0.440836\n",
      "Epoch [326/500]  Train Loss: 0.006357  Val Loss: 0.033538\n",
      "Epoch [327/500]  Train Loss: 0.007127  Val Loss: 0.382441\n",
      "Epoch [328/500]  Train Loss: 0.008137  Val Loss: 0.049261\n",
      "Epoch [329/500]  Train Loss: 0.008542  Val Loss: 2.253085\n",
      "Epoch [330/500]  Train Loss: 0.007394  Val Loss: 0.161028\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_330.pth\n",
      "Epoch [331/500]  Train Loss: 0.007099  Val Loss: 3.170548\n",
      "Epoch [332/500]  Train Loss: 0.008168  Val Loss: 1.772857\n",
      "Epoch [333/500]  Train Loss: 0.007348  Val Loss: 0.341089\n",
      "Epoch [334/500]  Train Loss: 0.007728  Val Loss: 0.065753\n",
      "Epoch [335/500]  Train Loss: 0.009550  Val Loss: 4.109984\n",
      "Epoch [336/500]  Train Loss: 0.007513  Val Loss: 0.151057\n",
      "Epoch [337/500]  Train Loss: 0.007385  Val Loss: 1.625165\n",
      "Epoch [338/500]  Train Loss: 0.007211  Val Loss: 0.693748\n",
      "Epoch [339/500]  Train Loss: 0.007190  Val Loss: 0.135810\n",
      "Epoch [340/500]  Train Loss: 0.006053  Val Loss: 0.647135\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_340.pth\n",
      "Epoch [341/500]  Train Loss: 0.008560  Val Loss: 0.564601\n",
      "Epoch [342/500]  Train Loss: 0.009252  Val Loss: 0.157191\n",
      "Epoch [343/500]  Train Loss: 0.009534  Val Loss: 0.833525\n",
      "Epoch [344/500]  Train Loss: 0.006212  Val Loss: 0.283665\n",
      "Epoch [345/500]  Train Loss: 0.009662  Val Loss: 0.670595\n",
      "Epoch [346/500]  Train Loss: 0.008494  Val Loss: 0.060271\n",
      "Epoch [347/500]  Train Loss: 0.007749  Val Loss: 4.527099\n",
      "Epoch [348/500]  Train Loss: 0.005177  Val Loss: 1.443374\n",
      "Epoch [349/500]  Train Loss: 0.006246  Val Loss: 1.237076\n",
      "Epoch [350/500]  Train Loss: 0.007049  Val Loss: 0.271733\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_350.pth\n",
      "Epoch [351/500]  Train Loss: 0.006404  Val Loss: 0.037092\n",
      "Epoch [352/500]  Train Loss: 0.008590  Val Loss: 1.333476\n",
      "Epoch [353/500]  Train Loss: 0.007330  Val Loss: 1.092968\n",
      "Epoch [354/500]  Train Loss: 0.007825  Val Loss: 0.574152\n",
      "Epoch [355/500]  Train Loss: 0.004952  Val Loss: 1.241947\n",
      "Epoch [356/500]  Train Loss: 0.006745  Val Loss: 0.031963\n",
      "Epoch [357/500]  Train Loss: 0.006588  Val Loss: 0.423597\n",
      "Epoch [358/500]  Train Loss: 0.011286  Val Loss: 1.299848\n",
      "Epoch [359/500]  Train Loss: 0.009974  Val Loss: 10.080344\n",
      "Epoch [360/500]  Train Loss: 0.007040  Val Loss: 2.872114\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_360.pth\n",
      "Epoch [361/500]  Train Loss: 0.011321  Val Loss: 0.459178\n",
      "Epoch [362/500]  Train Loss: 0.009069  Val Loss: 0.119384\n",
      "Epoch [363/500]  Train Loss: 0.006912  Val Loss: 1.007512\n",
      "Epoch [364/500]  Train Loss: 0.009482  Val Loss: 0.894076\n",
      "Epoch [365/500]  Train Loss: 0.010355  Val Loss: 0.627187\n",
      "Epoch [366/500]  Train Loss: 0.007603  Val Loss: 0.253893\n",
      "Epoch [367/500]  Train Loss: 0.007942  Val Loss: 0.055465\n",
      "Epoch [368/500]  Train Loss: 0.005005  Val Loss: 0.034000\n",
      "Epoch [369/500]  Train Loss: 0.005576  Val Loss: 0.101281\n",
      "Epoch [370/500]  Train Loss: 0.007552  Val Loss: 0.326047\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_370.pth\n",
      "Epoch [371/500]  Train Loss: 0.006537  Val Loss: 0.026753\n",
      "Epoch [372/500]  Train Loss: 0.006150  Val Loss: 0.708674\n",
      "Epoch [373/500]  Train Loss: 0.006894  Val Loss: 0.047787\n",
      "Epoch [374/500]  Train Loss: 0.006377  Val Loss: 4.895262\n",
      "Epoch [375/500]  Train Loss: 0.006438  Val Loss: 0.929108\n",
      "Epoch [376/500]  Train Loss: 0.005875  Val Loss: 0.384513\n",
      "Epoch [377/500]  Train Loss: 0.010115  Val Loss: 0.145112\n",
      "Epoch [378/500]  Train Loss: 0.007653  Val Loss: 0.327027\n",
      "Epoch [379/500]  Train Loss: 0.006499  Val Loss: 2.581401\n",
      "Epoch [380/500]  Train Loss: 0.005494  Val Loss: 0.043800\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_380.pth\n",
      "Epoch [381/500]  Train Loss: 0.006454  Val Loss: 0.959617\n",
      "Epoch [382/500]  Train Loss: 0.005904  Val Loss: 0.876079\n",
      "Epoch [383/500]  Train Loss: 0.012055  Val Loss: 2.491439\n",
      "Epoch [384/500]  Train Loss: 0.010234  Val Loss: 0.209953\n",
      "Epoch [385/500]  Train Loss: 0.005474  Val Loss: 0.124443\n",
      "Epoch [386/500]  Train Loss: 0.008042  Val Loss: 0.174847\n",
      "Epoch [387/500]  Train Loss: 0.007043  Val Loss: 0.197703\n",
      "Epoch [388/500]  Train Loss: 0.007370  Val Loss: 0.695991\n",
      "Epoch [389/500]  Train Loss: 0.006941  Val Loss: 0.666214\n",
      "Epoch [390/500]  Train Loss: 0.007734  Val Loss: 0.055877\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_390.pth\n",
      "Epoch [391/500]  Train Loss: 0.006785  Val Loss: 0.063275\n",
      "Epoch [392/500]  Train Loss: 0.005191  Val Loss: 0.021490\n",
      "Epoch [393/500]  Train Loss: 0.006513  Val Loss: 3.000029\n",
      "Epoch [394/500]  Train Loss: 0.006695  Val Loss: 0.716506\n",
      "Epoch [395/500]  Train Loss: 0.005941  Val Loss: 1.869878\n",
      "Epoch [396/500]  Train Loss: 0.007885  Val Loss: 3.304737\n",
      "Epoch [397/500]  Train Loss: 0.006867  Val Loss: 0.678084\n",
      "Epoch [398/500]  Train Loss: 0.011267  Val Loss: 0.980084\n",
      "Epoch [399/500]  Train Loss: 0.007361  Val Loss: 2.553250\n",
      "Epoch [400/500]  Train Loss: 0.005498  Val Loss: 1.176799\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_400.pth\n",
      "Epoch [401/500]  Train Loss: 0.005292  Val Loss: 1.261199\n",
      "Epoch [402/500]  Train Loss: 0.005522  Val Loss: 0.179139\n",
      "Epoch [403/500]  Train Loss: 0.005486  Val Loss: 0.251836\n",
      "Epoch [404/500]  Train Loss: 0.005769  Val Loss: 1.037380\n",
      "Epoch [405/500]  Train Loss: 0.004398  Val Loss: 1.079352\n",
      "Epoch [406/500]  Train Loss: 0.005599  Val Loss: 0.193682\n",
      "Epoch [407/500]  Train Loss: 0.005963  Val Loss: 0.239867\n",
      "Epoch [408/500]  Train Loss: 0.005722  Val Loss: 0.022613\n",
      "Epoch [409/500]  Train Loss: 0.006416  Val Loss: 0.237709\n",
      "Epoch [410/500]  Train Loss: 0.005237  Val Loss: 0.159816\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_410.pth\n",
      "Epoch [411/500]  Train Loss: 0.006229  Val Loss: 1.404359\n",
      "Epoch [412/500]  Train Loss: 0.007487  Val Loss: 0.727068\n",
      "Epoch [413/500]  Train Loss: 0.006367  Val Loss: 0.385055\n",
      "Epoch [414/500]  Train Loss: 0.006195  Val Loss: 2.444529\n",
      "Epoch [415/500]  Train Loss: 0.007982  Val Loss: 0.933430\n",
      "Epoch [416/500]  Train Loss: 0.005468  Val Loss: 0.413362\n",
      "Epoch [417/500]  Train Loss: 0.006022  Val Loss: 0.918061\n",
      "Epoch [418/500]  Train Loss: 0.007068  Val Loss: 0.032625\n",
      "Epoch [419/500]  Train Loss: 0.006453  Val Loss: 0.664523\n",
      "Epoch [420/500]  Train Loss: 0.008282  Val Loss: 0.883438\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_420.pth\n",
      "Epoch [421/500]  Train Loss: 0.005301  Val Loss: 0.404923\n",
      "Epoch [422/500]  Train Loss: 0.006205  Val Loss: 0.380050\n",
      "Epoch [423/500]  Train Loss: 0.005658  Val Loss: 1.522165\n",
      "Epoch [424/500]  Train Loss: 0.006421  Val Loss: 0.245180\n",
      "Epoch [425/500]  Train Loss: 0.007058  Val Loss: 0.686839\n",
      "Epoch [426/500]  Train Loss: 0.007774  Val Loss: 0.333310\n",
      "Epoch [427/500]  Train Loss: 0.008334  Val Loss: 0.048126\n",
      "Epoch [428/500]  Train Loss: 0.006557  Val Loss: 0.874959\n",
      "Epoch [429/500]  Train Loss: 0.007020  Val Loss: 4.255757\n",
      "Epoch [430/500]  Train Loss: 0.007322  Val Loss: 1.441792\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_430.pth\n",
      "Epoch [431/500]  Train Loss: 0.006639  Val Loss: 1.970143\n",
      "Epoch [432/500]  Train Loss: 0.006071  Val Loss: 0.187742\n",
      "Epoch [433/500]  Train Loss: 0.005668  Val Loss: 0.332726\n",
      "Epoch [434/500]  Train Loss: 0.005366  Val Loss: 0.272235\n",
      "Epoch [435/500]  Train Loss: 0.005832  Val Loss: 0.090661\n",
      "Epoch [436/500]  Train Loss: 0.004930  Val Loss: 1.128736\n",
      "Epoch [437/500]  Train Loss: 0.004972  Val Loss: 0.215490\n",
      "Epoch [438/500]  Train Loss: 0.006930  Val Loss: 0.200453\n",
      "Epoch [439/500]  Train Loss: 0.008549  Val Loss: 1.584790\n",
      "Epoch [440/500]  Train Loss: 0.005646  Val Loss: 1.080709\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_440.pth\n",
      "Epoch [441/500]  Train Loss: 0.006462  Val Loss: 1.742441\n",
      "Epoch [442/500]  Train Loss: 0.004882  Val Loss: 0.394357\n",
      "Epoch [443/500]  Train Loss: 0.006092  Val Loss: 0.676088\n",
      "Epoch [444/500]  Train Loss: 0.005817  Val Loss: 2.033803\n",
      "Epoch [445/500]  Train Loss: 0.006614  Val Loss: 0.218572\n",
      "Epoch [446/500]  Train Loss: 0.005542  Val Loss: 0.741891\n",
      "Epoch [447/500]  Train Loss: 0.007838  Val Loss: 0.938988\n",
      "Epoch [448/500]  Train Loss: 0.007674  Val Loss: 0.457608\n",
      "Epoch [449/500]  Train Loss: 0.009018  Val Loss: 0.268937\n",
      "Epoch [450/500]  Train Loss: 0.007239  Val Loss: 1.115633\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_450.pth\n",
      "Epoch [451/500]  Train Loss: 0.004703  Val Loss: 0.269349\n",
      "Epoch [452/500]  Train Loss: 0.005080  Val Loss: 0.218585\n",
      "Epoch [453/500]  Train Loss: 0.004374  Val Loss: 0.443686\n",
      "Epoch [454/500]  Train Loss: 0.005476  Val Loss: 0.344399\n",
      "Epoch [455/500]  Train Loss: 0.005044  Val Loss: 0.168561\n",
      "Epoch [456/500]  Train Loss: 0.006893  Val Loss: 0.644144\n",
      "Epoch [457/500]  Train Loss: 0.008657  Val Loss: 1.191450\n",
      "Epoch [458/500]  Train Loss: 0.005888  Val Loss: 0.241689\n",
      "Epoch [459/500]  Train Loss: 0.007456  Val Loss: 4.430814\n",
      "Epoch [460/500]  Train Loss: 0.006705  Val Loss: 3.189473\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_460.pth\n",
      "Epoch [461/500]  Train Loss: 0.005917  Val Loss: 0.036274\n",
      "Epoch [462/500]  Train Loss: 0.005490  Val Loss: 0.160001\n",
      "Epoch [463/500]  Train Loss: 0.005444  Val Loss: 3.237625\n",
      "Epoch [464/500]  Train Loss: 0.004533  Val Loss: 1.131943\n",
      "Epoch [465/500]  Train Loss: 0.004787  Val Loss: 1.143965\n",
      "Epoch [466/500]  Train Loss: 0.004881  Val Loss: 2.951429\n",
      "Epoch [467/500]  Train Loss: 0.007359  Val Loss: 0.248151\n",
      "Epoch [468/500]  Train Loss: 0.005443  Val Loss: 2.976716\n",
      "Epoch [469/500]  Train Loss: 0.006127  Val Loss: 1.791280\n",
      "Epoch [470/500]  Train Loss: 0.005087  Val Loss: 0.962920\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_470.pth\n",
      "Epoch [471/500]  Train Loss: 0.006160  Val Loss: 0.504879\n",
      "Epoch [472/500]  Train Loss: 0.004475  Val Loss: 0.060505\n",
      "Epoch [473/500]  Train Loss: 0.006505  Val Loss: 0.431043\n",
      "Epoch [474/500]  Train Loss: 0.004316  Val Loss: 0.370709\n",
      "Epoch [475/500]  Train Loss: 0.006156  Val Loss: 2.180345\n",
      "Epoch [476/500]  Train Loss: 0.005384  Val Loss: 0.198243\n",
      "Epoch [477/500]  Train Loss: 0.006047  Val Loss: 0.201378\n",
      "Epoch [478/500]  Train Loss: 0.006552  Val Loss: 0.266596\n",
      "Epoch [479/500]  Train Loss: 0.005841  Val Loss: 0.101541\n",
      "Epoch [480/500]  Train Loss: 0.006109  Val Loss: 0.859090\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_480.pth\n",
      "Epoch [481/500]  Train Loss: 0.006498  Val Loss: 0.109935\n",
      "Epoch [482/500]  Train Loss: 0.006463  Val Loss: 0.541019\n",
      "Epoch [483/500]  Train Loss: 0.005004  Val Loss: 0.451262\n",
      "Epoch [484/500]  Train Loss: 0.006727  Val Loss: 0.249664\n",
      "Epoch [485/500]  Train Loss: 0.006219  Val Loss: 0.113676\n",
      "Epoch [486/500]  Train Loss: 0.005923  Val Loss: 1.021417\n",
      "Epoch [487/500]  Train Loss: 0.007692  Val Loss: 0.282408\n",
      "Epoch [488/500]  Train Loss: 0.010469  Val Loss: 0.048847\n",
      "Epoch [489/500]  Train Loss: 0.007087  Val Loss: 0.343809\n",
      "Epoch [490/500]  Train Loss: 0.006179  Val Loss: 0.173989\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_490.pth\n",
      "Epoch [491/500]  Train Loss: 0.007672  Val Loss: 0.261709\n",
      "Epoch [492/500]  Train Loss: 0.004605  Val Loss: 0.487209\n",
      "Epoch [493/500]  Train Loss: 0.005818  Val Loss: 3.771812\n",
      "Epoch [494/500]  Train Loss: 0.005793  Val Loss: 0.109261\n",
      "Epoch [495/500]  Train Loss: 0.008191  Val Loss: 0.023482\n",
      "Epoch [496/500]  Train Loss: 0.005308  Val Loss: 0.459084\n",
      "Epoch [497/500]  Train Loss: 0.004775  Val Loss: 1.187874\n",
      "Epoch [498/500]  Train Loss: 0.004447  Val Loss: 0.637080\n",
      "Epoch [499/500]  Train Loss: 0.004376  Val Loss: 0.762693\n",
      "Epoch [500/500]  Train Loss: 0.003781  Val Loss: 0.715945\n",
      "Saved checkpoint: checkpoints/pc_dc_pn/checkpoint_epoch_500.pth\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for points, cd in train_loader:\n",
    "        points = points.to(DEVICE)\n",
    "        cd = cd.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(points)\n",
    "        loss = criterion(outputs, cd)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * points.size(0)\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss_total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for points, cd in val_loader:\n",
    "            points = points.to(DEVICE)\n",
    "            cd = cd.to(DEVICE)\n",
    "            \n",
    "            outputs = model(points)\n",
    "            val_loss_total += criterion(outputs, cd).item() * points.size(0)\n",
    "    \n",
    "    val_loss = val_loss_total / len(val_loader.dataset)\n",
    "    \n",
    "    # Logging\n",
    "    log_line = f\"Epoch [{epoch+1}/{NUM_EPOCHS}]  Train Loss: {train_loss:.6f}  Val Loss: {val_loss:.6f}\"\n",
    "    print(log_line)\n",
    "    \n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(log_line + \"\\n\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % SAVE_EVERY == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }, ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d16976e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_models/pointnet_cd_regressor.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"saved_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save the model weights\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"pointnet_cd_regressor.pth\")\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model saved to: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61282589",
   "metadata": {},
   "source": [
    "## Inference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "770a3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# This contains the min/max Cd from the training set\n",
    "with open(\"02_dataset/cd_data_303/drag_normalization.json\") as f:\n",
    "    norm_metadata = json.load(f)\n",
    "\n",
    "CD_MIN = norm_metadata[\"Cd_min\"]\n",
    "CD_MAX = norm_metadata[\"Cd_max\"]\n",
    "\n",
    "def inverse_normalize(cd_norm):\n",
    "    return cd_norm * (CD_MAX - CD_MIN) + CD_MIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40114049",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Important!\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b89c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for points, cd in test_loader:\n",
    "        points = points.to(DEVICE)\n",
    "        cd = cd.to(DEVICE)\n",
    "        \n",
    "        outputs = model(points)  # normalized Cd\n",
    "        all_preds.append(outputs.cpu())\n",
    "        all_true.append(cd.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "all_true = torch.cat(all_true, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_phys = inverse_normalize(all_preds)\n",
    "all_true_phys = inverse_normalize(all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a2458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE (Cd): 0.064325\n",
      "Test RMSE (Cd): 0.066412\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "diff = all_preds_phys - all_true_phys\n",
    "mae = np.mean(np.abs(diff))\n",
    "rmse = np.sqrt(np.mean(diff ** 2))\n",
    "\n",
    "print(f\"Test MAE (Cd): {mae:.6f}\")\n",
    "print(f\"Test RMSE (Cd): {rmse:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
